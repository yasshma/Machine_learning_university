{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Классификация изображений с помощью сверточных нейронных сетей**\n\nВ данном задании Вам необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6).\nЗаготовка кода для выполнения задания приведена выше. Вашей задачей будет заполнить пропущеные места, которые отмечены ключевым словом *None*.\nНеобходимая точность (accuracy) сети на датасете CIFAR100 - 30%\nЖелаемая точность (accuracy) сети на датасете CIFAR100 - 45%","metadata":{}},{"cell_type":"code","source":"!pip install keras.datasets","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:29:33.246351Z","iopub.execute_input":"2023-01-25T19:29:33.246777Z","iopub.status.idle":"2023-01-25T19:29:43.455611Z","shell.execute_reply.started":"2023-01-25T19:29:33.246738Z","shell.execute_reply":"2023-01-25T19:29:43.454357Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting keras.datasets\n  Downloading keras_datasets-0.1.0-py3-none-any.whl (4.2 kB)\nInstalling collected packages: keras.datasets\nSuccessfully installed keras.datasets-0.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras-flops","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:28:45.252622Z","iopub.execute_input":"2023-01-25T19:28:45.253281Z","iopub.status.idle":"2023-01-25T19:29:33.244083Z","shell.execute_reply.started":"2023-01-25T19:28:45.253175Z","shell.execute_reply":"2023-01-25T19:29:33.242895Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-flops\n  Downloading keras_flops-0.1.2-py3-none-any.whl (5.3 kB)\nRequirement already satisfied: tensorflow<3.0,>=2.2 in /opt/conda/lib/python3.7/site-packages (from keras-flops) (2.6.4)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.15.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (5.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12.1)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<3.0,>=2.2->keras-flops) (1.5.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3.7)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.28.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.13.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.26.13)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, keras-flops\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.8.6 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytoolconfig 1.2.4 requires typing-extensions>=4.4.0; python_version < \"3.8\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.32.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nimbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.0.1 which is incompatible.\nflax 0.6.3 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncupy-cuda110 11.4.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\ncmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.44 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 keras-flops-0.1.2 numpy-1.19.5 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Импорт необходимых библиотек\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.datasets\nfrom keras_flops import get_flops\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:30:24.302876Z","iopub.execute_input":"2023-01-25T19:30:24.303255Z","iopub.status.idle":"2023-01-25T19:30:32.339585Z","shell.execute_reply.started":"2023-01-25T19:30:24.303223Z","shell.execute_reply":"2023-01-25T19:30:32.338628Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nCLASSES       = 100\nBATCH_SIZE    = 128\nLEARNING_RATE = 1e-2","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:30:49.250715Z","iopub.execute_input":"2023-01-25T19:30:49.251315Z","iopub.status.idle":"2023-01-25T19:30:49.256795Z","shell.execute_reply.started":"2023-01-25T19:30:49.251276Z","shell.execute_reply":"2023-01-25T19:30:49.255783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cifar100 = keras.datasets.cifar100.load_data(label_mode=\"fine\")\n(x_train, y_train), (x_val, y_val) = cifar100\nassert x_train.shape == (50000, 32, 32, 3)\nassert x_val.shape == (10000, 32, 32, 3)\nassert y_train.shape == (50000, 1)\nassert y_val.shape == (10000, 1)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:30:53.650463Z","iopub.execute_input":"2023-01-25T19:30:53.650852Z","iopub.status.idle":"2023-01-25T19:40:50.710459Z","shell.execute_reply.started":"2023-01-25T19:30:53.650818Z","shell.execute_reply":"2023-01-25T19:40:50.709382Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n169009152/169001437 [==============================] - 594s 4us/step\n169017344/169001437 [==============================] - 594s 4us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"y_train = keras.utils.np_utils.to_categorical(y_train, CLASSES)\ny_val = keras.utils.np_utils.to_categorical(y_val, CLASSES)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:41:00.782253Z","iopub.execute_input":"2023-01-25T19:41:00.782617Z","iopub.status.idle":"2023-01-25T19:41:00.803606Z","shell.execute_reply.started":"2023-01-25T19:41:00.782586Z","shell.execute_reply":"2023-01-25T19:41:00.802583Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Задайте архитектуру модели\nmodel = tf.keras.models.Sequential([\n    \n    tf.keras.Input(shape=[32,32,3]),\n    tf.keras.layers.Conv2D(32, 3, padding='same', strides=3),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.1),\n    \n    tf.keras.layers.Conv2D(64, 3, padding='same', strides=3),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('relu'), \n    tf.keras.layers.Dropout(0.1),\n    \n    tf.keras.layers.Conv2D(128, 2, padding='same', strides=2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.Dropout(0.1),\n\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Activation('softmax')\n    \n])","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:41:20.307337Z","iopub.execute_input":"2023-01-25T19:41:20.307731Z","iopub.status.idle":"2023-01-25T19:41:25.643717Z","shell.execute_reply.started":"2023-01-25T19:41:20.307693Z","shell.execute_reply":"2023-01-25T19:41:25.642825Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2023-01-25 19:41:20.476612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.477598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.655748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.656591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.657356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.658060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.661544: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-25 19:41:20.916366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.917245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.918018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.918786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.919486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:20.920195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.019969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.020883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.021591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.022333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.023083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.023826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-01-25 19:41:25.028326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:25.029087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"# вычисление количества операций\nflops = get_flops(model, batch_size=1)\nprint(f\"FLOPs: {(flops / 1e6):.4f}e6\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:41:55.092280Z","iopub.execute_input":"2023-01-25T19:41:55.092679Z","iopub.status.idle":"2023-01-25T19:41:55.248042Z","shell.execute_reply.started":"2023-01-25T19:41:55.092629Z","shell.execute_reply":"2023-01-25T19:41:55.246385Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfiFLOPs: 0.4995e6\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 19:41:55.149032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.149757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.150486: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n2023-01-25 19:41:55.150601: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-25 19:41:55.151019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.151506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.152330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.152832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.153570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.154021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.154897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.155301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.156008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.156370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-01-25 19:41:55.156466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:41:55.157104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n2023-01-25 19:41:55.159588: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n\n","output_type":"stream"},{"name":"stdout","text":"le:\nnode name | # float_ops\n_TFProfRoot (--/499.51k flops)\n  sequential/conv2d/Conv2D (209.09k/209.09k flops)\n  sequential/conv2d_1/Conv2D (147.46k/147.46k flops)\n  sequential/conv2d_2/Conv2D (65.54k/65.54k flops)\n  sequential/dense/MatMul (32.77k/32.77k flops)\n  sequential/dense_1/MatMul (25.60k/25.60k flops)\n  sequential/batch_normalization/FusedBatchNormV3 (7.94k/7.94k flops)\n  sequential/conv2d/BiasAdd (3.87k/3.87k flops)\n  sequential/max_pooling2d/MaxPool (3.20k/3.20k flops)\n  sequential/batch_normalization_2/FusedBatchNormV3 (1.02k/1.02k flops)\n  sequential/batch_normalization_1/FusedBatchNormV3 (896/896 flops)\n  sequential/activation_4/Softmax (500/500 flops)\n  sequential/conv2d_1/BiasAdd (256/256 flops)\n  sequential/batch_normalization_3/batchnorm/Rsqrt (256/256 flops)\n  sequential/batch_normalization_3/batchnorm/mul (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/mul_1 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/mul_2 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/sub (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/add_1 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/add (128/128 flops)\n  sequential/conv2d_2/BiasAdd (128/128 flops)\n  sequential/dense/BiasAdd (128/128 flops)\n  sequential/dense_1/BiasAdd (100/100 flops)\n\n======================End of Report==========================\n","output_type":"stream"}]},{"cell_type":"code","source":"# вывод краткой информации о модели\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:42:00.323044Z","iopub.execute_input":"2023-01-25T19:42:00.323604Z","iopub.status.idle":"2023-01-25T19:42:00.332474Z","shell.execute_reply.started":"2023-01-25T19:42:00.323571Z","shell.execute_reply":"2023-01-25T19:42:00.331142Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 11, 11, 32)        896       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 11, 11, 32)        128       \n_________________________________________________________________\nactivation (Activation)      (None, 11, 11, 32)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 5, 5, 32)          0         \n_________________________________________________________________\ndropout (Dropout)            (None, 5, 5, 32)          0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 2, 2, 64)          18496     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 2, 2, 64)          256       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 2, 2, 64)          0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 2, 2, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 1, 1, 128)         32896     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 1, 1, 128)         512       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 1, 1, 128)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 1, 1, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 128)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               16512     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 128)               512       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 128)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               12900     \n_________________________________________________________________\nactivation_4 (Activation)    (None, 100)               0         \n=================================================================\nTotal params: 83,108\nTrainable params: 82,404\nNon-trainable params: 704\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n    optimizer = tf.keras.optimizers.SGD(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-2,\n    decay_steps=10000,\n    decay_rate=0.9)),\n    loss=tf.keras.losses.CategoricalCrossentropy(),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:42:12.387861Z","iopub.execute_input":"2023-01-25T19:42:12.388317Z","iopub.status.idle":"2023-01-25T19:42:12.416525Z","shell.execute_reply.started":"2023-01-25T19:42:12.388279Z","shell.execute_reply":"2023-01-25T19:42:12.414992Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nmodel.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    batch_size=BATCH_SIZE,\n    callbacks=[\n        tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),\n        \n    ],\n    use_multiprocessing=True,\n    workers=8,\n    epochs=256\n    \n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:43:00.007643Z","iopub.execute_input":"2023-01-25T19:43:00.008070Z","iopub.status.idle":"2023-01-25T19:53:00.389793Z","shell.execute_reply.started":"2023-01-25T19:43:00.008037Z","shell.execute_reply":"2023-01-25T19:53:00.388677Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2023-01-25 19:43:00.919389: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/256\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 19:43:03.439800: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"391/391 [==============================] - 15s 8ms/step - loss: 4.6270 - accuracy: 0.0266 - val_loss: 4.3097 - val_accuracy: 0.0603\nEpoch 2/256\n391/391 [==============================] - 2s 6ms/step - loss: 4.3125 - accuracy: 0.0546 - val_loss: 4.1216 - val_accuracy: 0.0910\nEpoch 3/256\n391/391 [==============================] - 3s 7ms/step - loss: 4.1515 - accuracy: 0.0769 - val_loss: 3.9681 - val_accuracy: 0.1169\nEpoch 4/256\n391/391 [==============================] - 2s 6ms/step - loss: 4.0415 - accuracy: 0.0910 - val_loss: 3.8900 - val_accuracy: 0.1312\nEpoch 5/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.9474 - accuracy: 0.1036 - val_loss: 3.8208 - val_accuracy: 0.1383\nEpoch 6/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.8759 - accuracy: 0.1127 - val_loss: 3.7541 - val_accuracy: 0.1506\nEpoch 7/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.8123 - accuracy: 0.1244 - val_loss: 3.6864 - val_accuracy: 0.1606\nEpoch 8/256\n391/391 [==============================] - 2s 5ms/step - loss: 3.7572 - accuracy: 0.1307 - val_loss: 3.7000 - val_accuracy: 0.1505\nEpoch 9/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.7087 - accuracy: 0.1369 - val_loss: 3.6127 - val_accuracy: 0.1607\nEpoch 10/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.6606 - accuracy: 0.1429 - val_loss: 3.5722 - val_accuracy: 0.1682\nEpoch 11/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.6199 - accuracy: 0.1509 - val_loss: 3.5378 - val_accuracy: 0.1769\nEpoch 12/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.5787 - accuracy: 0.1556 - val_loss: 3.4310 - val_accuracy: 0.1997\nEpoch 13/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.5462 - accuracy: 0.1639 - val_loss: 3.4322 - val_accuracy: 0.1971\nEpoch 14/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.5116 - accuracy: 0.1651 - val_loss: 3.3722 - val_accuracy: 0.2045\nEpoch 15/256\n391/391 [==============================] - 3s 7ms/step - loss: 3.4762 - accuracy: 0.1712 - val_loss: 3.3349 - val_accuracy: 0.2149\nEpoch 16/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.4472 - accuracy: 0.1774 - val_loss: 3.4776 - val_accuracy: 0.1825\nEpoch 17/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.4208 - accuracy: 0.1805 - val_loss: 3.2794 - val_accuracy: 0.2182\nEpoch 18/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.3930 - accuracy: 0.1852 - val_loss: 3.2455 - val_accuracy: 0.2292\nEpoch 19/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.3667 - accuracy: 0.1900 - val_loss: 3.3320 - val_accuracy: 0.2090\nEpoch 20/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.3412 - accuracy: 0.1947 - val_loss: 3.1885 - val_accuracy: 0.2388\nEpoch 21/256\n391/391 [==============================] - 3s 6ms/step - loss: 3.3153 - accuracy: 0.1983 - val_loss: 3.2053 - val_accuracy: 0.2324\nEpoch 22/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.2939 - accuracy: 0.2025 - val_loss: 3.1338 - val_accuracy: 0.2469\nEpoch 23/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.2742 - accuracy: 0.2082 - val_loss: 3.1644 - val_accuracy: 0.2353\nEpoch 24/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.2542 - accuracy: 0.2081 - val_loss: 3.1232 - val_accuracy: 0.2439\nEpoch 25/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.2298 - accuracy: 0.2148 - val_loss: 3.0844 - val_accuracy: 0.2535\nEpoch 26/256\n391/391 [==============================] - 3s 7ms/step - loss: 3.2150 - accuracy: 0.2174 - val_loss: 3.0958 - val_accuracy: 0.2529\nEpoch 27/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.1938 - accuracy: 0.2180 - val_loss: 3.0495 - val_accuracy: 0.2583\nEpoch 28/256\n391/391 [==============================] - 3s 7ms/step - loss: 3.1720 - accuracy: 0.2223 - val_loss: 3.1497 - val_accuracy: 0.2323\nEpoch 29/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.1605 - accuracy: 0.2287 - val_loss: 3.0324 - val_accuracy: 0.2597\nEpoch 30/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.1407 - accuracy: 0.2285 - val_loss: 2.9839 - val_accuracy: 0.2676\nEpoch 31/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.1218 - accuracy: 0.2310 - val_loss: 3.1126 - val_accuracy: 0.2445\nEpoch 32/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.1064 - accuracy: 0.2357 - val_loss: 2.9766 - val_accuracy: 0.2685\nEpoch 33/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0868 - accuracy: 0.2376 - val_loss: 3.1347 - val_accuracy: 0.2367\nEpoch 34/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0760 - accuracy: 0.2402 - val_loss: 2.9413 - val_accuracy: 0.2779\nEpoch 35/256\n391/391 [==============================] - 3s 6ms/step - loss: 3.0609 - accuracy: 0.2431 - val_loss: 3.0380 - val_accuracy: 0.2609\nEpoch 36/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0439 - accuracy: 0.2448 - val_loss: 2.9729 - val_accuracy: 0.2726\nEpoch 37/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0431 - accuracy: 0.2480 - val_loss: 2.9510 - val_accuracy: 0.2787\nEpoch 38/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0195 - accuracy: 0.2517 - val_loss: 2.9672 - val_accuracy: 0.2695\nEpoch 39/256\n391/391 [==============================] - 2s 6ms/step - loss: 3.0046 - accuracy: 0.2531 - val_loss: 2.8970 - val_accuracy: 0.2851\nEpoch 40/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.9959 - accuracy: 0.2567 - val_loss: 3.0058 - val_accuracy: 0.2561\nEpoch 41/256\n391/391 [==============================] - 3s 6ms/step - loss: 2.9855 - accuracy: 0.2573 - val_loss: 2.8444 - val_accuracy: 0.2954\nEpoch 42/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9750 - accuracy: 0.2591 - val_loss: 2.9175 - val_accuracy: 0.2801\nEpoch 43/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9625 - accuracy: 0.2622 - val_loss: 2.9476 - val_accuracy: 0.2679\nEpoch 44/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9487 - accuracy: 0.2640 - val_loss: 3.0518 - val_accuracy: 0.2535\nEpoch 45/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9379 - accuracy: 0.2659 - val_loss: 2.9583 - val_accuracy: 0.2718\nEpoch 46/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9253 - accuracy: 0.2685 - val_loss: 2.8446 - val_accuracy: 0.2944\nEpoch 47/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9223 - accuracy: 0.2677 - val_loss: 2.8451 - val_accuracy: 0.2912\nEpoch 48/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.9104 - accuracy: 0.2701 - val_loss: 2.8793 - val_accuracy: 0.2798\nEpoch 49/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8987 - accuracy: 0.2740 - val_loss: 3.0229 - val_accuracy: 0.2595\nEpoch 50/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8936 - accuracy: 0.2741 - val_loss: 2.8171 - val_accuracy: 0.2975\nEpoch 51/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8866 - accuracy: 0.2745 - val_loss: 2.7672 - val_accuracy: 0.3102\nEpoch 52/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8726 - accuracy: 0.2799 - val_loss: 2.8783 - val_accuracy: 0.2818\nEpoch 53/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8661 - accuracy: 0.2779 - val_loss: 2.7653 - val_accuracy: 0.3033\nEpoch 54/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8541 - accuracy: 0.2851 - val_loss: 2.7592 - val_accuracy: 0.3104\nEpoch 55/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.8458 - accuracy: 0.2843 - val_loss: 2.7936 - val_accuracy: 0.3043\nEpoch 56/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8399 - accuracy: 0.2845 - val_loss: 2.7370 - val_accuracy: 0.3144\nEpoch 57/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8362 - accuracy: 0.2825 - val_loss: 2.7542 - val_accuracy: 0.3098\nEpoch 58/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8184 - accuracy: 0.2908 - val_loss: 2.7345 - val_accuracy: 0.3129\nEpoch 59/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8146 - accuracy: 0.2900 - val_loss: 2.7427 - val_accuracy: 0.3096\nEpoch 60/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8093 - accuracy: 0.2886 - val_loss: 2.7312 - val_accuracy: 0.3155\nEpoch 61/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.8037 - accuracy: 0.2915 - val_loss: 2.8121 - val_accuracy: 0.2970\nEpoch 62/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7973 - accuracy: 0.2920 - val_loss: 2.7237 - val_accuracy: 0.3165\nEpoch 63/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7934 - accuracy: 0.2945 - val_loss: 2.8875 - val_accuracy: 0.2803\nEpoch 64/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7796 - accuracy: 0.2965 - val_loss: 2.8340 - val_accuracy: 0.2922\nEpoch 65/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7780 - accuracy: 0.2938 - val_loss: 3.0176 - val_accuracy: 0.2570\nEpoch 66/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7744 - accuracy: 0.2980 - val_loss: 2.6721 - val_accuracy: 0.3263\nEpoch 67/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7649 - accuracy: 0.3012 - val_loss: 2.6836 - val_accuracy: 0.3230\nEpoch 68/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7631 - accuracy: 0.2996 - val_loss: 2.7078 - val_accuracy: 0.3186\nEpoch 69/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.7491 - accuracy: 0.3015 - val_loss: 2.6904 - val_accuracy: 0.3187\nEpoch 70/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7513 - accuracy: 0.3042 - val_loss: 2.6614 - val_accuracy: 0.3252\nEpoch 71/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7411 - accuracy: 0.3028 - val_loss: 2.6347 - val_accuracy: 0.3293\nEpoch 72/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7372 - accuracy: 0.3050 - val_loss: 2.6208 - val_accuracy: 0.3347\nEpoch 73/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7358 - accuracy: 0.3075 - val_loss: 2.6226 - val_accuracy: 0.3347\nEpoch 74/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7255 - accuracy: 0.3047 - val_loss: 2.6303 - val_accuracy: 0.3294\nEpoch 75/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7180 - accuracy: 0.3088 - val_loss: 2.6443 - val_accuracy: 0.3258\nEpoch 76/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7123 - accuracy: 0.3099 - val_loss: 2.6109 - val_accuracy: 0.3393\nEpoch 77/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7054 - accuracy: 0.3109 - val_loss: 2.6862 - val_accuracy: 0.3220\nEpoch 78/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7076 - accuracy: 0.3121 - val_loss: 2.7008 - val_accuracy: 0.3208\nEpoch 79/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.7009 - accuracy: 0.3141 - val_loss: 2.6597 - val_accuracy: 0.3285\nEpoch 80/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6982 - accuracy: 0.3114 - val_loss: 2.6938 - val_accuracy: 0.3165\nEpoch 81/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6948 - accuracy: 0.3118 - val_loss: 2.6342 - val_accuracy: 0.3311\nEpoch 82/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6901 - accuracy: 0.3125 - val_loss: 2.6219 - val_accuracy: 0.3320\nEpoch 83/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.6759 - accuracy: 0.3165 - val_loss: 2.6283 - val_accuracy: 0.3319\nEpoch 84/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6820 - accuracy: 0.3160 - val_loss: 2.6651 - val_accuracy: 0.3235\nEpoch 85/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.6673 - accuracy: 0.3194 - val_loss: 2.6322 - val_accuracy: 0.3313\nEpoch 86/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6684 - accuracy: 0.3186 - val_loss: 2.6247 - val_accuracy: 0.3347\nEpoch 87/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6660 - accuracy: 0.3177 - val_loss: 2.5798 - val_accuracy: 0.3437\nEpoch 88/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6596 - accuracy: 0.3200 - val_loss: 2.7112 - val_accuracy: 0.3199\nEpoch 89/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6635 - accuracy: 0.3185 - val_loss: 2.8007 - val_accuracy: 0.2972\nEpoch 90/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.6535 - accuracy: 0.3227 - val_loss: 2.5847 - val_accuracy: 0.3434\nEpoch 91/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6532 - accuracy: 0.3198 - val_loss: 2.6489 - val_accuracy: 0.3277\nEpoch 92/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.6494 - accuracy: 0.3255 - val_loss: 2.6186 - val_accuracy: 0.3387\nEpoch 93/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6419 - accuracy: 0.3233 - val_loss: 2.6111 - val_accuracy: 0.3430\nEpoch 94/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6404 - accuracy: 0.3223 - val_loss: 2.6030 - val_accuracy: 0.3374\nEpoch 95/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6345 - accuracy: 0.3251 - val_loss: 2.5611 - val_accuracy: 0.3506\nEpoch 96/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6312 - accuracy: 0.3289 - val_loss: 2.6233 - val_accuracy: 0.3392\nEpoch 97/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.6344 - accuracy: 0.3254 - val_loss: 2.6320 - val_accuracy: 0.3313\nEpoch 98/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6260 - accuracy: 0.3263 - val_loss: 2.5652 - val_accuracy: 0.3427\nEpoch 99/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6250 - accuracy: 0.3259 - val_loss: 2.6065 - val_accuracy: 0.3402\nEpoch 100/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6211 - accuracy: 0.3268 - val_loss: 2.5569 - val_accuracy: 0.3473\nEpoch 101/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6122 - accuracy: 0.3298 - val_loss: 2.5503 - val_accuracy: 0.3516\nEpoch 102/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6086 - accuracy: 0.3307 - val_loss: 2.6799 - val_accuracy: 0.3199\nEpoch 103/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.6157 - accuracy: 0.3284 - val_loss: 2.5265 - val_accuracy: 0.3546\nEpoch 104/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.6018 - accuracy: 0.3299 - val_loss: 2.5493 - val_accuracy: 0.3510\nEpoch 105/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5967 - accuracy: 0.3331 - val_loss: 2.6094 - val_accuracy: 0.3433\nEpoch 106/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5974 - accuracy: 0.3324 - val_loss: 2.5504 - val_accuracy: 0.3494\nEpoch 107/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5882 - accuracy: 0.3340 - val_loss: 2.5001 - val_accuracy: 0.3642\nEpoch 108/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5906 - accuracy: 0.3350 - val_loss: 2.5862 - val_accuracy: 0.3435\nEpoch 109/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5919 - accuracy: 0.3328 - val_loss: 2.5597 - val_accuracy: 0.3482\nEpoch 110/256\n391/391 [==============================] - 3s 6ms/step - loss: 2.5921 - accuracy: 0.3338 - val_loss: 2.6287 - val_accuracy: 0.3333\nEpoch 111/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5818 - accuracy: 0.3330 - val_loss: 2.5147 - val_accuracy: 0.3575\nEpoch 112/256\n391/391 [==============================] - 3s 6ms/step - loss: 2.5833 - accuracy: 0.3346 - val_loss: 2.5802 - val_accuracy: 0.3405\nEpoch 113/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5757 - accuracy: 0.3358 - val_loss: 2.6407 - val_accuracy: 0.3320\nEpoch 114/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5734 - accuracy: 0.3374 - val_loss: 2.5542 - val_accuracy: 0.3513\nEpoch 115/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.5705 - accuracy: 0.3352 - val_loss: 2.4948 - val_accuracy: 0.3623\nEpoch 116/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5713 - accuracy: 0.3370 - val_loss: 2.5001 - val_accuracy: 0.3637\nEpoch 117/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5651 - accuracy: 0.3377 - val_loss: 2.6385 - val_accuracy: 0.3316\nEpoch 118/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5632 - accuracy: 0.3394 - val_loss: 2.4850 - val_accuracy: 0.3658\nEpoch 119/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5642 - accuracy: 0.3377 - val_loss: 2.5102 - val_accuracy: 0.3635\nEpoch 120/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5651 - accuracy: 0.3399 - val_loss: 2.5206 - val_accuracy: 0.3582\nEpoch 121/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5617 - accuracy: 0.3388 - val_loss: 2.5617 - val_accuracy: 0.3498\nEpoch 122/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5585 - accuracy: 0.3392 - val_loss: 2.4700 - val_accuracy: 0.3741\nEpoch 123/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5467 - accuracy: 0.3442 - val_loss: 2.4782 - val_accuracy: 0.3686\nEpoch 124/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.5491 - accuracy: 0.3429 - val_loss: 2.6669 - val_accuracy: 0.3288\nEpoch 125/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5427 - accuracy: 0.3424 - val_loss: 2.4885 - val_accuracy: 0.3652\nEpoch 126/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5501 - accuracy: 0.3410 - val_loss: 2.4842 - val_accuracy: 0.3635\nEpoch 127/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5422 - accuracy: 0.3444 - val_loss: 2.5471 - val_accuracy: 0.3568\nEpoch 128/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.5356 - accuracy: 0.3464 - val_loss: 2.5189 - val_accuracy: 0.3600\nEpoch 129/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5348 - accuracy: 0.3440 - val_loss: 2.4809 - val_accuracy: 0.3667\nEpoch 130/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.5438 - accuracy: 0.3424 - val_loss: 2.4871 - val_accuracy: 0.3652\nEpoch 131/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5335 - accuracy: 0.3458 - val_loss: 2.4833 - val_accuracy: 0.3640\nEpoch 132/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5326 - accuracy: 0.3439 - val_loss: 2.4873 - val_accuracy: 0.3618\nEpoch 133/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5252 - accuracy: 0.3457 - val_loss: 2.5047 - val_accuracy: 0.3629\nEpoch 134/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5284 - accuracy: 0.3457 - val_loss: 2.4546 - val_accuracy: 0.3743\nEpoch 135/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5183 - accuracy: 0.3449 - val_loss: 2.5152 - val_accuracy: 0.3560\nEpoch 136/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5227 - accuracy: 0.3470 - val_loss: 2.4998 - val_accuracy: 0.3569\nEpoch 137/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5276 - accuracy: 0.3490 - val_loss: 2.4777 - val_accuracy: 0.3649\nEpoch 138/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.5210 - accuracy: 0.3484 - val_loss: 2.5613 - val_accuracy: 0.3463\nEpoch 139/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5160 - accuracy: 0.3490 - val_loss: 2.4644 - val_accuracy: 0.3715\nEpoch 140/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5110 - accuracy: 0.3514 - val_loss: 2.4774 - val_accuracy: 0.3670\nEpoch 141/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5128 - accuracy: 0.3503 - val_loss: 2.4861 - val_accuracy: 0.3644\nEpoch 142/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5021 - accuracy: 0.3506 - val_loss: 2.6186 - val_accuracy: 0.3419\nEpoch 143/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5059 - accuracy: 0.3509 - val_loss: 2.4867 - val_accuracy: 0.3650\nEpoch 144/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5047 - accuracy: 0.3513 - val_loss: 2.4452 - val_accuracy: 0.3720\nEpoch 145/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5050 - accuracy: 0.3524 - val_loss: 2.4652 - val_accuracy: 0.3704\nEpoch 146/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.5015 - accuracy: 0.3517 - val_loss: 2.4434 - val_accuracy: 0.3759\nEpoch 147/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4969 - accuracy: 0.3533 - val_loss: 2.5132 - val_accuracy: 0.3599\nEpoch 148/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4991 - accuracy: 0.3512 - val_loss: 2.4739 - val_accuracy: 0.3669\nEpoch 149/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4897 - accuracy: 0.3536 - val_loss: 2.4414 - val_accuracy: 0.3781\nEpoch 150/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4915 - accuracy: 0.3551 - val_loss: 2.5457 - val_accuracy: 0.3583\nEpoch 151/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4887 - accuracy: 0.3539 - val_loss: 2.4415 - val_accuracy: 0.3768\nEpoch 152/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4905 - accuracy: 0.3548 - val_loss: 2.4567 - val_accuracy: 0.3712\nEpoch 153/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4901 - accuracy: 0.3539 - val_loss: 2.4513 - val_accuracy: 0.3717\nEpoch 154/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4885 - accuracy: 0.3538 - val_loss: 2.4757 - val_accuracy: 0.3696\nEpoch 155/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4829 - accuracy: 0.3558 - val_loss: 2.4272 - val_accuracy: 0.3784\nEpoch 156/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4805 - accuracy: 0.3558 - val_loss: 2.4559 - val_accuracy: 0.3694\nEpoch 157/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4758 - accuracy: 0.3571 - val_loss: 2.4415 - val_accuracy: 0.3732\nEpoch 158/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4773 - accuracy: 0.3575 - val_loss: 2.4321 - val_accuracy: 0.3781\nEpoch 159/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4720 - accuracy: 0.3583 - val_loss: 2.4471 - val_accuracy: 0.3720\nEpoch 160/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4734 - accuracy: 0.3553 - val_loss: 2.4627 - val_accuracy: 0.3696\nEpoch 161/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4701 - accuracy: 0.3568 - val_loss: 2.4406 - val_accuracy: 0.3780\nEpoch 162/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4691 - accuracy: 0.3588 - val_loss: 2.5281 - val_accuracy: 0.3560\nEpoch 163/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4660 - accuracy: 0.3568 - val_loss: 2.4511 - val_accuracy: 0.3755\nEpoch 164/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4657 - accuracy: 0.3599 - val_loss: 2.4242 - val_accuracy: 0.3791\nEpoch 165/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4696 - accuracy: 0.3576 - val_loss: 2.4930 - val_accuracy: 0.3644\nEpoch 166/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4615 - accuracy: 0.3604 - val_loss: 2.4248 - val_accuracy: 0.3811\nEpoch 167/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4653 - accuracy: 0.3581 - val_loss: 2.4573 - val_accuracy: 0.3712\nEpoch 168/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4692 - accuracy: 0.3580 - val_loss: 2.4659 - val_accuracy: 0.3770\nEpoch 169/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4586 - accuracy: 0.3611 - val_loss: 2.4370 - val_accuracy: 0.3752\nEpoch 170/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4608 - accuracy: 0.3582 - val_loss: 2.4435 - val_accuracy: 0.3769\nEpoch 171/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4585 - accuracy: 0.3601 - val_loss: 2.4324 - val_accuracy: 0.3766\nEpoch 172/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4544 - accuracy: 0.3589 - val_loss: 2.4417 - val_accuracy: 0.3743\nEpoch 173/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4566 - accuracy: 0.3617 - val_loss: 2.4347 - val_accuracy: 0.3787\nEpoch 174/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4517 - accuracy: 0.3593 - val_loss: 2.4306 - val_accuracy: 0.3748\nEpoch 175/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4508 - accuracy: 0.3613 - val_loss: 2.4099 - val_accuracy: 0.3829\nEpoch 176/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4472 - accuracy: 0.3637 - val_loss: 2.4252 - val_accuracy: 0.3779\nEpoch 177/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4471 - accuracy: 0.3622 - val_loss: 2.4314 - val_accuracy: 0.3786\nEpoch 178/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4466 - accuracy: 0.3646 - val_loss: 2.4240 - val_accuracy: 0.3767\nEpoch 179/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4368 - accuracy: 0.3658 - val_loss: 2.4727 - val_accuracy: 0.3721\nEpoch 180/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4455 - accuracy: 0.3640 - val_loss: 2.4074 - val_accuracy: 0.3810\nEpoch 181/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4408 - accuracy: 0.3668 - val_loss: 2.4920 - val_accuracy: 0.3674\nEpoch 182/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4422 - accuracy: 0.3643 - val_loss: 2.4886 - val_accuracy: 0.3688\nEpoch 183/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4438 - accuracy: 0.3640 - val_loss: 2.4248 - val_accuracy: 0.3811\nEpoch 184/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4406 - accuracy: 0.3652 - val_loss: 2.4775 - val_accuracy: 0.3644\nEpoch 185/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4405 - accuracy: 0.3635 - val_loss: 2.4114 - val_accuracy: 0.3799\nEpoch 186/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4405 - accuracy: 0.3658 - val_loss: 2.5523 - val_accuracy: 0.3484\nEpoch 187/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4291 - accuracy: 0.3658 - val_loss: 2.4331 - val_accuracy: 0.3786\nEpoch 188/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4389 - accuracy: 0.3676 - val_loss: 2.4380 - val_accuracy: 0.3720\nEpoch 189/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4331 - accuracy: 0.3658 - val_loss: 2.4435 - val_accuracy: 0.3737\nEpoch 190/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4291 - accuracy: 0.3674 - val_loss: 2.5187 - val_accuracy: 0.3614\nEpoch 191/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4355 - accuracy: 0.3648 - val_loss: 2.5057 - val_accuracy: 0.3642\nEpoch 192/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4253 - accuracy: 0.3673 - val_loss: 2.4355 - val_accuracy: 0.3788\nEpoch 193/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4242 - accuracy: 0.3661 - val_loss: 2.4372 - val_accuracy: 0.3743\nEpoch 194/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4273 - accuracy: 0.3678 - val_loss: 2.4437 - val_accuracy: 0.3720\nEpoch 195/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4184 - accuracy: 0.3669 - val_loss: 2.4701 - val_accuracy: 0.3713\nEpoch 196/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4239 - accuracy: 0.3679 - val_loss: 2.4425 - val_accuracy: 0.3744\nEpoch 197/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4229 - accuracy: 0.3687 - val_loss: 2.4154 - val_accuracy: 0.3804\nEpoch 198/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4142 - accuracy: 0.3700 - val_loss: 2.4283 - val_accuracy: 0.3802\nEpoch 199/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4290 - accuracy: 0.3669 - val_loss: 2.3981 - val_accuracy: 0.3838\nEpoch 200/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4171 - accuracy: 0.3708 - val_loss: 2.4216 - val_accuracy: 0.3773\nEpoch 201/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4134 - accuracy: 0.3702 - val_loss: 2.3971 - val_accuracy: 0.3854\nEpoch 202/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4126 - accuracy: 0.3721 - val_loss: 2.4210 - val_accuracy: 0.3804\nEpoch 203/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4217 - accuracy: 0.3683 - val_loss: 2.4006 - val_accuracy: 0.3839\nEpoch 204/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4071 - accuracy: 0.3715 - val_loss: 2.4324 - val_accuracy: 0.3758\nEpoch 205/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4146 - accuracy: 0.3704 - val_loss: 2.4046 - val_accuracy: 0.3794\nEpoch 206/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4091 - accuracy: 0.3717 - val_loss: 2.4419 - val_accuracy: 0.3772\nEpoch 207/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4070 - accuracy: 0.3730 - val_loss: 2.4470 - val_accuracy: 0.3723\nEpoch 208/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.4088 - accuracy: 0.3719 - val_loss: 2.3947 - val_accuracy: 0.3866\nEpoch 209/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3968 - accuracy: 0.3745 - val_loss: 2.4293 - val_accuracy: 0.3801\nEpoch 210/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.4020 - accuracy: 0.3730 - val_loss: 2.4179 - val_accuracy: 0.3838\nEpoch 211/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4067 - accuracy: 0.3714 - val_loss: 2.4214 - val_accuracy: 0.3796\nEpoch 212/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4072 - accuracy: 0.3725 - val_loss: 2.4109 - val_accuracy: 0.3815\nEpoch 213/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4058 - accuracy: 0.3705 - val_loss: 2.4479 - val_accuracy: 0.3773\nEpoch 214/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4008 - accuracy: 0.3711 - val_loss: 2.4004 - val_accuracy: 0.3844\nEpoch 215/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4064 - accuracy: 0.3707 - val_loss: 2.4066 - val_accuracy: 0.3853\nEpoch 216/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3986 - accuracy: 0.3720 - val_loss: 2.3940 - val_accuracy: 0.3845\nEpoch 217/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4078 - accuracy: 0.3718 - val_loss: 2.3977 - val_accuracy: 0.3854\nEpoch 218/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4022 - accuracy: 0.3717 - val_loss: 2.4181 - val_accuracy: 0.3790\nEpoch 219/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.4073 - accuracy: 0.3692 - val_loss: 2.3868 - val_accuracy: 0.3878\nEpoch 220/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3970 - accuracy: 0.3715 - val_loss: 2.4044 - val_accuracy: 0.3844\nEpoch 221/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3961 - accuracy: 0.3738 - val_loss: 2.3900 - val_accuracy: 0.3869\nEpoch 222/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3896 - accuracy: 0.3751 - val_loss: 2.3888 - val_accuracy: 0.3870\nEpoch 223/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3917 - accuracy: 0.3729 - val_loss: 2.4313 - val_accuracy: 0.3728\nEpoch 224/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3881 - accuracy: 0.3749 - val_loss: 2.4053 - val_accuracy: 0.3811\nEpoch 225/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3859 - accuracy: 0.3752 - val_loss: 2.4071 - val_accuracy: 0.3855\nEpoch 226/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3922 - accuracy: 0.3740 - val_loss: 2.4109 - val_accuracy: 0.3890\nEpoch 227/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3914 - accuracy: 0.3759 - val_loss: 2.4217 - val_accuracy: 0.3815\nEpoch 228/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3946 - accuracy: 0.3739 - val_loss: 2.4218 - val_accuracy: 0.3810\nEpoch 229/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3852 - accuracy: 0.3771 - val_loss: 2.3837 - val_accuracy: 0.3865\nEpoch 230/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3886 - accuracy: 0.3744 - val_loss: 2.4412 - val_accuracy: 0.3750\nEpoch 231/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3843 - accuracy: 0.3764 - val_loss: 2.4104 - val_accuracy: 0.3828\nEpoch 232/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3844 - accuracy: 0.3785 - val_loss: 2.4509 - val_accuracy: 0.3782\nEpoch 233/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3862 - accuracy: 0.3756 - val_loss: 2.3933 - val_accuracy: 0.3850\nEpoch 234/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3793 - accuracy: 0.3784 - val_loss: 2.3859 - val_accuracy: 0.3865\nEpoch 235/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3786 - accuracy: 0.3768 - val_loss: 2.3768 - val_accuracy: 0.3902\nEpoch 236/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3761 - accuracy: 0.3765 - val_loss: 2.4009 - val_accuracy: 0.3842\nEpoch 237/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3747 - accuracy: 0.3792 - val_loss: 2.3803 - val_accuracy: 0.3860\nEpoch 238/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3822 - accuracy: 0.3756 - val_loss: 2.4491 - val_accuracy: 0.3752\nEpoch 239/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3786 - accuracy: 0.3771 - val_loss: 2.3884 - val_accuracy: 0.3876\nEpoch 240/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3700 - accuracy: 0.3777 - val_loss: 2.3902 - val_accuracy: 0.3867\nEpoch 241/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3819 - accuracy: 0.3779 - val_loss: 2.3871 - val_accuracy: 0.3882\nEpoch 242/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3789 - accuracy: 0.3756 - val_loss: 2.4587 - val_accuracy: 0.3729\nEpoch 243/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3782 - accuracy: 0.3791 - val_loss: 2.3993 - val_accuracy: 0.3843\nEpoch 244/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3681 - accuracy: 0.3785 - val_loss: 2.3915 - val_accuracy: 0.3841\nEpoch 245/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3790 - accuracy: 0.3788 - val_loss: 2.3862 - val_accuracy: 0.3879\nEpoch 246/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3731 - accuracy: 0.3805 - val_loss: 2.3894 - val_accuracy: 0.3843\nEpoch 247/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3702 - accuracy: 0.3786 - val_loss: 2.4050 - val_accuracy: 0.3803\nEpoch 248/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3724 - accuracy: 0.3791 - val_loss: 2.3732 - val_accuracy: 0.3886\nEpoch 249/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3647 - accuracy: 0.3814 - val_loss: 2.4072 - val_accuracy: 0.3815\nEpoch 250/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3756 - accuracy: 0.3793 - val_loss: 2.4141 - val_accuracy: 0.3828\nEpoch 251/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3699 - accuracy: 0.3796 - val_loss: 2.4021 - val_accuracy: 0.3837\nEpoch 252/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3654 - accuracy: 0.3808 - val_loss: 2.3791 - val_accuracy: 0.3881\nEpoch 253/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3705 - accuracy: 0.3802 - val_loss: 2.3861 - val_accuracy: 0.3856\nEpoch 254/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3619 - accuracy: 0.3809 - val_loss: 2.3893 - val_accuracy: 0.3861\nEpoch 255/256\n391/391 [==============================] - 2s 6ms/step - loss: 2.3674 - accuracy: 0.3795 - val_loss: 2.3793 - val_accuracy: 0.3848\nEpoch 256/256\n391/391 [==============================] - 2s 5ms/step - loss: 2.3608 - accuracy: 0.3794 - val_loss: 2.3854 - val_accuracy: 0.3891\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f69ec640810>"},"metadata":{}}]},{"cell_type":"code","source":"tf.get_logger().setLevel(\"ERROR\")\n\nmodel.load_weights(\"201-0.39.hdf5\")\nscore = model.evaluate(x_val, y_val, verbose=0)\nprint(f\"Test loss: {round(score[0], 3)}\")\nprint(f\"Test accuracy: {round(score[1], 3)}\")\n\nflops = get_flops(model)\nprint(f\"FLOPs: {(flops / 1e6):.4f}e6\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T19:53:46.599729Z","iopub.execute_input":"2023-01-25T19:53:46.600418Z","iopub.status.idle":"2023-01-25T19:53:47.938061Z","shell.execute_reply.started":"2023-01-25T19:53:46.600378Z","shell.execute_reply":"2023-01-25T19:53:47.937119Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Test loss: 2.397\nTest accuracy: 0.385\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/499.51k flops)\n  sequential/conv2d/Conv2D (209.09k/209.09k flops)\n  sequential/conv2d_1/Conv2D (147.46k/147.46k flops)\n  sequential/conv2d_2/Conv2D (65.54k/65.54k flops)\n  sequential/dense/MatMul (32.77k/32.77k flops)\n  sequential/dense_1/MatMul (25.60k/25.60k flops)\n  sequential/batch_normalization/FusedBatchNormV3 (7.94k/7.94k flops)\n  sequential/conv2d/BiasAdd (3.87k/3.87k flops)\n  sequential/max_pooling2d/MaxPool (3.20k/3.20k flops)\n  sequential/batch_normalization_2/FusedBatchNormV3 (1.02k/1.02k flops)\n  sequential/batch_normalization_1/FusedBatchNormV3 (896/896 flops)\n  sequential/activation_4/Softmax (500/500 flops)\n  sequential/conv2d_1/BiasAdd (256/256 flops)\n  sequential/batch_normalization_3/batchnorm/Rsqrt (256/256 flops)\n  sequential/batch_normalization_3/batchnorm/mul (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/mul_1 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/mul_2 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/sub (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/add_1 (128/128 flops)\n  sequential/batch_normalization_3/batchnorm/add (128/128 flops)\n  sequential/conv2d_2/BiasAdd (128/128 flops)\n  sequential/dense/BiasAdd (128/128 flops)\n  sequential/dense_1/BiasAdd (100/100 flops)\n\n======================End of Report==========================\nFLOPs: 0.4995e6\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 19:53:47.789346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.790089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.791117: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n2023-01-25 19:53:47.791220: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-25 19:53:47.792043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.792637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.793690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.794221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.795180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.795763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.796973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.797511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.798485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.798955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2023-01-25 19:53:47.799076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 19:53:47.799963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n2023-01-25 19:53:47.804855: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.008ms.\n  function_optimizer: function_optimizer did nothing. time = 0.005ms.\n\n","output_type":"stream"}]}]}